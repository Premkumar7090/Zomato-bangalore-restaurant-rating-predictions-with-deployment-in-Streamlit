# -*- coding: utf-8 -*-
"""zomato_final(deep_learning).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SL0qecQo4pj9yG3CO61kla0GOSyjuBYa
"""


import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("C:/Users/Admin/anaconda3/zomato.csv")
df.head()

df.describe()

df1=df.drop(['url','dish_liked','phone'],axis=1)

df1.duplicated().sum()
df1.drop_duplicates(inplace=True)

df1.isnull().sum()
df1.dropna(how='any',inplace=True)
df1.info()

df1=df1.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type','listed_in(city)':'city'})

df1['rate'].unique()

df1=df1.loc[df1.rate!='NEW']
df1=df1.loc[df1.rate!='-'].reset_index(drop=True)
noslash=lambda x: x.replace('/5','')if type(x)==np.str else x
df1.rate=df1.rate.apply(noslash).str.strip().astype('float')
df1['rate'].head()

df1['cost']=df1['cost'].astype(str)
df1['cost']=df1['cost'].apply(lambda x:x.replace(',','.'))
df1['cost']=df1['cost'].astype(float)

def encode(df1):
  for column in df1.columns[~df1.columns.isin(['rate','cost','votes'])]:
    df1[column]=df1[column].factorize()[0]
  return df1
df1_en=encode(df1.copy())

import time
from tqdm import tqdm

myList = ['aaa','bbb','ccc','ddd','eee']

for i in tqdm(myList):
    time.sleep(2)
    print(i)



df1_en[df1_en.apply(lambda x: np.abs(x - x.mean()) / x.std() < 3).all(axis=1)]

import pandas as pd

def remove_outliers(df, q=0.05):
    upper = df.quantile(1-q)
    lower = df.quantile(q)
    mask = (df < upper) & (df > lower)
    return mask

mask = remove_outliers(df1_en, 0.1)

print(df1_en)

df1_en.describe()

def outlier_detect(df):
    for i in df.describe().columns:
        Q1=df.describe().at['25%',i]
        Q3=df.describe().at['75%',i]
        IQR=Q3 - Q1
        LTV=Q1 - 1.5 * IQR
        UTV=Q3 + 1.5 * IQR
        x=np.array(df[i])
        p=[]
        for j in x:
            if j < LTV or j>UTV:
                p.append(df[i].median())
            else:
                p.append(j)
        df[i]=p
    return df

outlier_detect(df1_en)

df1_en.isnull()

import os
import seaborn as sns

import matplotlib.pyplot as plt
plt.style.use('ggplot')
import plotly.offline as py
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=False)
from wordcloud import WordCloud
from geopy.geocoders import Nominatim
from folium.plugins import HeatMap
import folium
from tqdm import tqdm
import re
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from nltk import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import matplotlib.colors as mcolors
from sklearn.manifold import TSNE
from gensim.models import word2vec
import nltk

all_ratings = []

for name,ratings in tqdm(zip(df['name'],df['reviews_list'])):
    ratings = eval(ratings)
    for score, doc in ratings:
        if score:
            score = score.strip("Rated").strip()
            doc = doc.strip('RATED').strip()
            score = float(score)
            all_ratings.append([name,score, doc])    #from stackoverflow



max_features=3000
tokenizer=Tokenizer(num_words=max_features,split=' ')
tokenizer.fit_on_texts(rating_df['review'].values)
X = tokenizer.texts_to_sequences(rating_df['review'].values)   #from stackoverflow
X = pad_sequences(X)

X

embed_dim = 32
lstm_out = 32

model = Sequential()
model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))
#model.add(SpatialDropout1D(0.4))
model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])
print(model.summary())

rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)

rating_df=pd.DataFrame(all_ratings,columns=['name','rating','review'])
rating_df['review']=rating_df['review'].apply(lambda x : re.sub('[^a-zA-Z0-9\s]',"",x))
rating_df['review']

Y = pd.get_dummies(rating_df['sent'].astype(int)).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

batch_size = 3200
model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size)



validation_size = 1500

X_validate = X_test[-validation_size:]
Y_validate = Y_test[-validation_size:]
X_test = X_test[:-validation_size]
Y_test = Y_test[:-validation_size]
score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print(score)
print(acc)



























sorted(df1_en)
q1, q3= np.percentile(df1_en.cuisines,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)

df1_en['cuisines'].values[df1_en['cuisines'].values > 1923] = 253

df1_en.describe()

df1_en.head()

df1_en.rest_type.max()
sorted(df1_en)
q1, q3= np.percentile(df1_en.rest_type,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)
df1_en['rest_type'].values[df1_en['rest_type'].values > 7] = 2
df1_en.rest_type.max()

sorted(df1_en)
q1, q3= np.percentile(df1_en.reviews_list,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(lower_bound) #30575
df1_en.reviews_list.min()

df1_en.menu_item.median()
sorted(df1_en)
q1, q3= np.percentile(df1_en.menu_item,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)
df1_en['menu_item'].values[df1_en['menu_item'].values > 7] = 2

df1_en.type.median()
sorted(df1_en)
q1, q3= np.percentile(df1_en.type,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)
df1_en['type'].values[df1_en['type'].values > 39.5] = 15
boxplot = df1_en.boxplot(column=['type'])

df1_en.city.median()
sorted(df1_en)
q1, q3= np.percentile(df1_en.city,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)
df1_en['city'].values[df1_en['city'].values > 39.5] = 15
boxplot = df1_en.boxplot(column=['city'])

df1_en.reviews_list.median() #7403.0
sorted(df1_en)
q1, q3= np.percentile(df1_en.reviews_list,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)
df1_en['reviews_list'].values[df1_en['reviews_list'].values > 30575] = 7403.0
boxplot = df1_en.boxplot(column=['reviews_list'])
df1_en.reviews_list.max()

sorted(df1_en)
q1, q3= np.percentile(df1_en.address,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound) #9728.5
df1_en['city'].values[df1_en['city'].values > 9728] = 3435
boxplot = df1_en.boxplot(column=['address'])

df1_en.address.median()
sorted(df1_en)
q1, q3= np.percentile(df1_en.address,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound)

df1_en.head()

#df1_en.address.votes()
sorted(df1_en)
q1, q3= np.percentile(df1_en.votes,[25,75])
iqr = q3 - q1
lower_bound = q1 -(1.5 * iqr) 
upper_bound = q3 +(1.5 * iqr)
print(upper_bound) #306
df1_en['votes'].values[df1_en['votes'].values > 306.0] = 73
boxplot = df1_en.boxplot(column="votes")

df1_en.votes.median()

df1_en.rate.max()

df1_en.to_csv('zomato_new.csv')

import pandas as pd

dff=pd.read_csv("zomato_new.csv")

dff



dff['sent']=dff['rate'].apply(lambda x: 1 if int(x)>2.5 else 0)

x = dff.iloc[:,[1,2,3,6,7,8,9,11]]
y = dff.rate
#Getting Test and Training Set
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)
x_train.head()
y_train.head()

print(x)

#Importing Libraries
import numpy as np #NumPy is a general-purpose array-processing package.
import pandas as pd #It contains high-level data structures and manipulation tools designed to make data analysis fast and easy.
import matplotlib.pyplot as plt #It is a Plotting Library
import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib.
from sklearn.linear_model import LogisticRegression #Logistic Regression is a Machine Learning classification algorithm
from sklearn.linear_model import LinearRegression #Linear Regression is a Machine Learning classification algorithm
from sklearn.model_selection import train_test_split #Splitting of Dataset
from sklearn.metrics import classification_report 
from sklearn.metrics import confusion_matrix
from sklearn.metrics import r2_score

#Prepare a Linear REgression Model
reg=LinearRegression()
reg.fit(x_train,y_train)
y_pred=reg.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

from sklearn.tree import DecisionTreeRegressor
#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)
DTree=DecisionTreeRegressor(min_samples_leaf=.0001)
DTree.fit(x_train,y_train)
y_predict=DTree.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_predict)

from sklearn.ensemble import RandomForestRegressor
RForest=RandomForestRegressor(n_estimators=500,random_state=329,min_samples_leaf=.0001)
RForest.fit(x_train,y_train)
y_predict=RForest.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_predict)

from sklearn.ensemble import  ExtraTreesRegressor
ETree=ExtraTreesRegressor(n_estimators = 100)
ETree.fit(x_train,y_train)
y_predict=ETree.predict(x_test)
from sklearn.metrics import r2_score
r2_score(y_test,y_predict)

import statsmodels.api as sm
from statsmodels.sandbox.regression.predstd import wls_prediction_std
model1=sm.OLS(y_train,x_train)
result=model1.fit()
print(result.summary())

from sklearn.linear_model import LinearRegression as lm
model=lm().fit(x_train,y_train)
predictions=model.predict(x_test)
import matplotlib.pyplot as plt
plt.scatter(y_test,predictions)
plt.xlabel('True values')
plt.ylabel('Predictions')

# evaluate knn performance on train and test sets with different numbers of neighbors
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from matplotlib import pyplot
from sklearn import preprocessing
from sklearn import utils

lab_enc = preprocessing.LabelEncoder()
encoded = lab_enc.fit_transform(x_train,y_train)
# create dataset
#X, y = make_classification(n_samples=10000, n_features=20, n_informative=5, n_redundant=15, random_state=1)
# split into train test sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# define lists to collect scores
train_scores, test_scores = list(), list()
# define the tree depths to evaluate
values = [i for i in range(1, 51)]
# evaluate a decision tree for each depth
for i in values:
	# configure the model
	model = KNeighborsClassifier(n_neighbors=i)
	# fit model on the training dataset
  
	model.fit(x_train, y_train)
	# evaluate on the train dataset
	train_yhat = model.predict(x_train)
	train_acc = accuracy_score(y_train, train_yhat)
	train_scores.append(train_acc)
	# evaluate on the test dataset
	test_yhat = model.predict(x_test)
	test_acc = accuracy_score(y_test, test_yhat)
	test_scores.append(test_acc)
	# summarize progress
	print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))
# plot of train and test scores vs number of neighbors
pyplot.plot(values, train_scores, '-o', label='Train')
pyplot.plot(values, test_scores, '-o', label='Test')
pyplot.legend()
pyplot.show()

